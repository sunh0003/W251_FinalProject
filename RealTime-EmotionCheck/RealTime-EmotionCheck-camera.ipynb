{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_top = load_model('3cls_224_ConfHappySur_Color_newimg_load.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " EMOTION_DICT = {1: 'happy', 0: 'confused', 2: 'surprise'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction: Input image, output argmax(prediction label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(path):\n",
    "    #converting image to gray scale and save it\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    \n",
    "    #detect face in image, crop it then resize it then save it\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_clip = img[y:y+h, x:x+w]\n",
    "        face_resize = cv2.resize(face_clip, (224,224))\n",
    "        cv2.imwrite(path, face_resize)\n",
    "    \n",
    "    #read the processed image then make prediction and display the result\n",
    "    read_image = cv2.imread(path) \n",
    "    read_image = read_image.reshape(1, read_image.shape[0], read_image.shape[1], read_image.shape[2])\n",
    "\n",
    "    top_pred = model_top.predict(read_image)\n",
    "    emotion_label = top_pred[0].argmax()\n",
    "    print (run, emotion_label , EMOTION_DICT[emotion_label])\n",
    "    return EMOTION_DICT[emotion_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Capture and trigger prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def first_run(text, cap):\n",
    "    while(True):\n",
    "        ret, img = cap.read()\n",
    "    \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, \"Emotion was \"+str(text), (95,30), font, 1.0, (117, 255, 51), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(img, \"Press SPACE: FOR EMOTION\", (5,470), font, 0.7, (117, 255, 51), 2, cv2.LINE_AA)\n",
    "    \n",
    "        cv2.putText(img, \"Hold Q: To Quit\", (460,470), font, 0.7, (117, 255, 51), 2, cv2.LINE_AA)\n",
    "    \n",
    "        faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "        for x,y,w,h in faces:\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        cv2.imshow(\"Emotion detector\", img)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord(' '):\n",
    "            cv2.imwrite(str(run) + \"_test.jpg\", img)\n",
    "            text = return_prediction(str(run) + \"_test.jpg\")\n",
    "            return text\n",
    "            \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image capture trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "emotionCount = []\n",
    "run = 0 \n",
    "for i in range(20):\n",
    "    run += 1\n",
    "    if i ==0 : \n",
    "        emotion_return = first_run(\"None\", cap)\n",
    "    else: \n",
    "        emotion_return = first_run(emotion_return, cap)\n",
    "    emotionCount.append(emotion_return)\n",
    "Counter(emotionCount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
